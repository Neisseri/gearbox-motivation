{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSV Reader\n",
    "\n",
    "`def read_large_csv(file_path)` parse CSV line by line.\n",
    "To use it, run `for row in read_large_csv(xxx):`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def read_large_csv(file_path):\n",
    "    with open(file_path, mode='r', encoding='utf-8') as f:\n",
    "        reader = csv.reader(f)\n",
    "        header = next(reader)\n",
    "        print(f\"CSV Header of {file_path}: {header}\")\n",
    "        for row in reader:\n",
    "            yield row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve Anomaly Groudtruth from log files\n",
    "\n",
    "Grountruth anomaly log files are from [../../Datasets/dataset2/data/run/run_table_2021-07.csv](../../Datasets/dataset2/data/run/run_table_2021-07.csv) and [../../Datasets/dataset2/data/run/run_table_2021-08.csv](../../Datasets/dataset2/data/run/run_table_2021-08.csv).\n",
    "The following scripts generates an **anomalies** list.\n",
    "Each of its elements is `(anomaly_type, start_timestamp, end_timestamp, duration)`.\n",
    "`anomaly_type` contains `Mem`, `Single Trace Wait`, `File`, `CPU`, `Access`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve Anomaly Groudtruth from the log files\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "injection_log_1 = \"../../Datasets/dataset2/data/run/run_table_2021-07.csv\"\n",
    "injection_log_2 = \"../../Datasets/dataset2/data/run/run_table_2021-08.csv\"\n",
    "\n",
    "# anomaly_types: Mem, Single Trace Wait, File, CPU, Access\n",
    "# anomalies = [(anomaly_type, start_timestamp, end_timestamp, duration), ...]\n",
    "anomalies = []\n",
    "anomaly_raw_cnt = 0\n",
    "\n",
    "for row in read_large_csv(injection_log_1):\n",
    "    log_entry = row[-1].split(' | ')\n",
    "\n",
    "    # These logs are mostly '(Background on this error at: http://sqlalche.me/e/14/e3q8)\\n'\n",
    "    if len(log_entry) < 2:\n",
    "        continue\n",
    "\n",
    "    log_type = log_entry[1]\n",
    "    if log_type == \"INFO\" or log_type == \"ERROR\":\n",
    "        continue\n",
    "    if log_type != \"WARNING\":\n",
    "        raise Exception(f\"Unknown log type: {log_type}\")\n",
    "\n",
    "    anomaly_raw_cnt += 1    \n",
    "    if \"[memory_anomalies]\" in log_entry[-1]:\n",
    "        anomaly_info = log_entry[-1]\n",
    "        anomaly_type = \"Mem\"\n",
    "\n",
    "        # 提取开始时间\n",
    "        start_indicator = \"start at \"\n",
    "        start_idx = anomaly_info.index(start_indicator) + len(start_indicator)\n",
    "        end_idx = anomaly_info.index(\" and lasts\", start_idx)\n",
    "        timestamp_str = anomaly_info[start_idx:end_idx]\n",
    "        start_ts = datetime.strptime(timestamp_str, \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "\n",
    "        lasts_indicator = \"lasts \"\n",
    "        lasts_idx = anomaly_info.index(lasts_indicator) + len(lasts_indicator)\n",
    "        lasts_end_idx = anomaly_info.index(\" seconds\", lasts_idx)\n",
    "        lasts_str = anomaly_info[lasts_idx:lasts_end_idx]\n",
    "        duration = float(lasts_str)\n",
    "\n",
    "        end_ts = start_ts + timedelta(seconds=duration)\n",
    "\n",
    "        # print(log_entry)\n",
    "        # print(f\"Anomaly: {start_ts} - {end_ts} ({duration}s)\")\n",
    "        anomalies.append((anomaly_type, start_ts, end_ts, duration))\n",
    "    elif \"[normal memory freed label]\" in log_entry[-1]:\n",
    "        continue\n",
    "    elif \"simulate the login failure of the QR code expired\" in log_entry[-1]:\n",
    "        anomaly_info = log_entry[-1]\n",
    "        anomaly_type = \"Single Trace Wait\"\n",
    "\n",
    "        # 提取开始时间\n",
    "        timestamp_sec_str = log_entry[0].split(',')[0]\n",
    "        start_ts = datetime.strptime(timestamp_sec_str, \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "        lasts_indicator = \"wait for \"\n",
    "        lasts_idx = anomaly_info.index(lasts_indicator) + len(lasts_indicator)\n",
    "        lasts_end_idx = anomaly_info.index(\" seconds\", lasts_idx)\n",
    "        lasts_str = anomaly_info[lasts_idx:lasts_end_idx]\n",
    "        duration = float(lasts_str)\n",
    "\n",
    "        end_ts = start_ts + timedelta(seconds=duration)\n",
    "\n",
    "        # print(log_entry)\n",
    "        # print(f\"Anomaly: {start_ts} - {end_ts} ({duration}s)\")\n",
    "        anomalies.append((anomaly_type, start_ts, end_ts, duration))\n",
    "    elif \"trigger the file moving program\" in log_entry[-1]:\n",
    "        anomaly_info = log_entry[-1]\n",
    "        anomaly_type = \"File\"\n",
    "\n",
    "        # 提取开始时间\n",
    "        start_indicator = \"start with \"\n",
    "        start_idx = anomaly_info.index(start_indicator) + len(start_indicator)\n",
    "        end_idx = anomaly_info.index(\", last for\", start_idx)\n",
    "        timestamp_str = anomaly_info[start_idx:end_idx]\n",
    "        start_ts = datetime.strptime(timestamp_str, \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "\n",
    "        lasts_indicator = \"last for \"\n",
    "        lasts_idx = anomaly_info.index(lasts_indicator) + len(lasts_indicator)\n",
    "        lasts_end_idx = anomaly_info.index(\" seconds\", lasts_idx)\n",
    "        lasts_str = anomaly_info[lasts_idx:lasts_end_idx]\n",
    "        duration = float(lasts_str)\n",
    "\n",
    "        end_ts = start_ts + timedelta(seconds=duration)\n",
    "\n",
    "        # print(log_entry)\n",
    "        # print(f\"Anomaly: {start_ts} - {end_ts} ({duration}s)\")\n",
    "        anomalies.append((anomaly_type, start_ts, end_ts, duration))\n",
    "    elif \"[cpu_anomalies]\" in log_entry[-1]:\n",
    "        anomaly_info = log_entry[-1]\n",
    "        anomaly_type = \"CPU\"\n",
    "\n",
    "        # 提取开始时间\n",
    "        start_indicator = \"start at \"\n",
    "        start_idx = anomaly_info.index(start_indicator) + len(start_indicator)\n",
    "        end_idx = anomaly_info.index(\" and lasts\", start_idx)\n",
    "        timestamp_str = anomaly_info[start_idx:end_idx]\n",
    "        start_ts = datetime.strptime(timestamp_str, \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "\n",
    "        lasts_indicator = \"lasts \"\n",
    "        lasts_idx = anomaly_info.index(lasts_indicator) + len(lasts_indicator)\n",
    "        lasts_end_idx = anomaly_info.index(\" seconds\", lasts_idx)\n",
    "        lasts_str = anomaly_info[lasts_idx:lasts_end_idx]\n",
    "        duration = float(lasts_str)\n",
    "\n",
    "        end_ts = start_ts + timedelta(seconds=duration)\n",
    "\n",
    "        # print(log_entry)\n",
    "        # print(f\"Anomaly: {start_ts} - {end_ts} ({duration}s)\")\n",
    "        if duration > 24 * 60 * 60 * 14:\n",
    "            print(f\"The anomaly is too long, ignored: {duration}s = {duration / 60 / 60}h = {duration / 60 / 60 / 24}d\")\n",
    "            continue\n",
    "        anomalies.append((anomaly_type, start_ts, end_ts, duration))\n",
    "    elif \"trigger an access permission denied exception, will lasts an hour\" in log_entry[-1]:\n",
    "        anomaly_type = \"Access\"\n",
    "\n",
    "        # 提取开始时间\n",
    "        timestamp_sec_str = log_entry[0].split(',')[0]\n",
    "        start_ts = datetime.strptime(timestamp_sec_str, \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "        duration = 60 * 60  # 1 Hour\n",
    "\n",
    "        end_ts = start_ts + timedelta(seconds=duration)        \n",
    "        # print(log_entry)\n",
    "        # print(f\"Anomaly: {start_ts} - {end_ts} ({duration}s)\")\n",
    "        anomalies.append((anomaly_type, start_ts, end_ts, duration))\n",
    "    else:\n",
    "        print(log_entry)\n",
    "\n",
    "for row in read_large_csv(injection_log_2):\n",
    "    log_entry = row[-1].split(' | ')\n",
    "\n",
    "    # These logs are mostly '(Background on this error at: http://sqlalche.me/e/14/e3q8)\\n'\n",
    "    if len(log_entry) < 2:\n",
    "        continue\n",
    "\n",
    "    log_type = log_entry[1]\n",
    "    if log_type == \"INFO\" or log_type == \"ERROR\":\n",
    "        continue\n",
    "    if log_type != \"WARNING\":\n",
    "        raise Exception(f\"Unknown log type: {log_type}\")\n",
    "\n",
    "    anomaly_raw_cnt += 1    \n",
    "    if \"[memory_anomalies]\" in log_entry[-1]:\n",
    "        anomaly_info = log_entry[-1]\n",
    "        anomaly_type = \"Mem\"\n",
    "\n",
    "        # 提取开始时间\n",
    "        start_indicator = \"start at \"\n",
    "        start_idx = anomaly_info.index(start_indicator) + len(start_indicator)\n",
    "        end_idx = anomaly_info.index(\" and lasts\", start_idx)\n",
    "        timestamp_str = anomaly_info[start_idx:end_idx]\n",
    "        start_ts = datetime.strptime(timestamp_str, \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "\n",
    "        lasts_indicator = \"lasts \"\n",
    "        lasts_idx = anomaly_info.index(lasts_indicator) + len(lasts_indicator)\n",
    "        lasts_end_idx = anomaly_info.index(\" seconds\", lasts_idx)\n",
    "        lasts_str = anomaly_info[lasts_idx:lasts_end_idx]\n",
    "        duration = float(lasts_str)\n",
    "\n",
    "        end_ts = start_ts + timedelta(seconds=duration)\n",
    "\n",
    "        # print(log_entry)\n",
    "        # print(f\"Anomaly: {start_ts} - {end_ts} ({duration}s)\")\n",
    "        anomalies.append((anomaly_type, start_ts, end_ts, duration))\n",
    "    elif \"[normal memory freed label]\" in log_entry[-1]:\n",
    "        continue\n",
    "    elif \"simulate the login failure of the QR code expired\" in log_entry[-1]:\n",
    "        anomaly_info = log_entry[-1]\n",
    "        anomaly_type = \"Single Trace Wait\"\n",
    "\n",
    "        # 提取开始时间\n",
    "        timestamp_sec_str = log_entry[0].split(',')[0]\n",
    "        start_ts = datetime.strptime(timestamp_sec_str, \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "        lasts_indicator = \"wait for \"\n",
    "        lasts_idx = anomaly_info.index(lasts_indicator) + len(lasts_indicator)\n",
    "        lasts_end_idx = anomaly_info.index(\" seconds\", lasts_idx)\n",
    "        lasts_str = anomaly_info[lasts_idx:lasts_end_idx]\n",
    "        duration = float(lasts_str)\n",
    "\n",
    "        end_ts = start_ts + timedelta(seconds=duration)\n",
    "\n",
    "        # print(log_entry)\n",
    "        # print(f\"Anomaly: {start_ts} - {end_ts} ({duration}s)\")\n",
    "        anomalies.append((anomaly_type, start_ts, end_ts, duration))\n",
    "    elif \"trigger the file moving program\" in log_entry[-1]:\n",
    "        anomaly_info = log_entry[-1]\n",
    "        anomaly_type = \"File\"\n",
    "\n",
    "        # 提取开始时间\n",
    "        start_indicator = \"start with \"\n",
    "        start_idx = anomaly_info.index(start_indicator) + len(start_indicator)\n",
    "        end_idx = anomaly_info.index(\", last for\", start_idx)\n",
    "        timestamp_str = anomaly_info[start_idx:end_idx]\n",
    "        start_ts = datetime.strptime(timestamp_str, \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "\n",
    "        lasts_indicator = \"last for \"\n",
    "        lasts_idx = anomaly_info.index(lasts_indicator) + len(lasts_indicator)\n",
    "        lasts_end_idx = anomaly_info.index(\" seconds\", lasts_idx)\n",
    "        lasts_str = anomaly_info[lasts_idx:lasts_end_idx]\n",
    "        duration = float(lasts_str)\n",
    "\n",
    "        end_ts = start_ts + timedelta(seconds=duration)\n",
    "\n",
    "        # print(log_entry)\n",
    "        # print(f\"Anomaly: {start_ts} - {end_ts} ({duration}s)\")\n",
    "        anomalies.append((anomaly_type, start_ts, end_ts, duration))\n",
    "    elif \"[cpu_anomalies]\" in log_entry[-1]:\n",
    "        anomaly_info = log_entry[-1]\n",
    "        anomaly_type = \"CPU\"\n",
    "\n",
    "        # 提取开始时间\n",
    "        start_indicator = \"start at \"\n",
    "        start_idx = anomaly_info.index(start_indicator) + len(start_indicator)\n",
    "        end_idx = anomaly_info.index(\" and lasts\", start_idx)\n",
    "        timestamp_str = anomaly_info[start_idx:end_idx]\n",
    "        start_ts = datetime.strptime(timestamp_str, \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "\n",
    "        lasts_indicator = \"lasts \"\n",
    "        lasts_idx = anomaly_info.index(lasts_indicator) + len(lasts_indicator)\n",
    "        lasts_end_idx = anomaly_info.index(\" seconds\", lasts_idx)\n",
    "        lasts_str = anomaly_info[lasts_idx:lasts_end_idx]\n",
    "        duration = float(lasts_str)\n",
    "\n",
    "        end_ts = start_ts + timedelta(seconds=duration)\n",
    "\n",
    "        # print(log_entry)\n",
    "        # print(f\"Anomaly: {start_ts} - {end_ts} ({duration}s)\")\n",
    "        if duration > 24 * 60 * 60 * 14:\n",
    "            print(f\"The anomaly is too long, ignored: {duration}s = {duration / 60 / 60}h = {duration / 60 / 60 / 24}d\")\n",
    "            continue\n",
    "        anomalies.append((anomaly_type, start_ts, end_ts, duration))\n",
    "    elif \"trigger an access permission denied exception, will lasts an hour\" in log_entry[-1]:\n",
    "        anomaly_type = \"Access\"\n",
    "\n",
    "        # 提取开始时间\n",
    "        timestamp_sec_str = log_entry[0].split(',')[0]\n",
    "        start_ts = datetime.strptime(timestamp_sec_str, \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "        duration = 60 * 60  # 1 Hour\n",
    "\n",
    "        end_ts = start_ts + timedelta(seconds=duration)        \n",
    "        # print(log_entry)\n",
    "        # print(f\"Anomaly: {start_ts} - {end_ts} ({duration}s)\")\n",
    "        anomalies.append((anomaly_type, start_ts, end_ts, duration))\n",
    "    else:\n",
    "        print(log_entry)\n",
    "\n",
    "print(f\"Anomaly count: {anomaly_raw_cnt}, {len(anomalies)} anomalies found.\")\n",
    "# for anomaly in anomalies:\n",
    "#     print(anomaly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve End-to-end Anomaly Symptom from trace files\n",
    "\n",
    "Grountruth anomaly log files are from `../../Datasets/dataset2/data/trace/*.csv`.\n",
    "The following scripts generates an **e2e_anomalies** list.\n",
    "Each of its elements is `(start_timestamp, end_timestamp)`.\n",
    "Anomalies are judged by 3-sigma principle ($\\mu - 3 * \\sigma \\leq value \\leq \\mu + 3 * \\sigma$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "def list_files(root_dir):\n",
    "    for root, dirs, files in os.walk(root_dir):\n",
    "        for file in files:\n",
    "            yield os.path.join(root, file)\n",
    "\n",
    "# Generate trace first\n",
    "trace_dir = \"../../Datasets/dataset2/data/trace/\"\n",
    "trace_dict = {}\n",
    "for trace_file in list_files(trace_dir):\n",
    "    print(f\"Processing {trace_file}\")\n",
    "    row_count = 0\n",
    "    for row in read_large_csv(trace_file):\n",
    "        row_count += 1\n",
    "        if row_count % 500000 == 0:\n",
    "            print(f\"Processed {row_count} rows\")\n",
    "\n",
    "        service_name = row[2]\n",
    "        trace_id = row[3]\n",
    "        span_id = row[4]\n",
    "        parent_span_id = row[5]\n",
    "        try:\n",
    "            start_ts = datetime.strptime(row[6], \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "        except ValueError:\n",
    "            start_ts = datetime.strptime(row[6], \"%Y-%m-%d %H:%M:%S\")\n",
    "        try:\n",
    "            end_ts = datetime.strptime(row[7], \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "        except ValueError:\n",
    "            end_ts = datetime.strptime(row[7], \"%Y-%m-%d %H:%M:%S\")\n",
    "        status_code = int(row[9])\n",
    "\n",
    "        if trace_id not in trace_dict:\n",
    "            trace_dict[trace_id] = [(service_name, span_id, parent_span_id, start_ts, end_ts, status_code)]\n",
    "        else:\n",
    "            trace_dict[trace_id].append((service_name, span_id, parent_span_id, start_ts, end_ts, status_code))            \n",
    "\n",
    "print(f\"Trace count: {len(trace_dict)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_topology(span_list):\n",
    "    def add_span_to_topology(topology, span):\n",
    "        service, span_id, parent_id, _, _, _ = span\n",
    "        if parent_id not in topology:\n",
    "            topology[parent_id] = {'children': [], 'service': \"Start\"}\n",
    "        if span_id not in topology:\n",
    "            topology[span_id] = {'service': service, 'children': []}\n",
    "        topology[parent_id]['children'].append(span_id)\n",
    "        topology[span_id]['service'] = service\n",
    "\n",
    "    topology = {}\n",
    "    for span in span_list:\n",
    "        add_span_to_topology(topology, span)\n",
    "    return topology\n",
    "\n",
    "def normalize_topology(topology, root_id='0'):\n",
    "    \"\"\"\n",
    "    Standardize the topology for comparison by normalizing the ordering.\n",
    "    \"\"\"\n",
    "    def recursive_normalize(node):\n",
    "        if 'service' in node:\n",
    "            node_str = node['service']\n",
    "        else:\n",
    "            node_str = \"\"\n",
    "        children_strs = []\n",
    "        for child_id in node['children']:\n",
    "            if child_id in topology:\n",
    "                child_str = recursive_normalize(topology[child_id])\n",
    "                children_strs.append(child_str)\n",
    "        children_strs.sort()\n",
    "        complete_str = node_str + \"(\" + \",\".join(children_strs) + \")\"\n",
    "        return complete_str\n",
    "\n",
    "    root_topology = topology.get(root_id, {'service': '', 'children': []})\n",
    "    return recursive_normalize(root_topology)\n",
    "\n",
    "e2e_latency = dict()\n",
    "error_code_set = set()\n",
    "for trace_id, spans in trace_dict.items():\n",
    "    topo = build_topology(spans)\n",
    "    norm_topo = normalize_topology(topo)\n",
    "    start_ts, end_ts, ret_code = None, None, None\n",
    "    for span in spans:\n",
    "        # print(span)\n",
    "        error_code_set.add(span[-1])\n",
    "\n",
    "        if ret_code is None or ret_code < span[-1]:\n",
    "            ret_code = span[-1]\n",
    "        if start_ts is None or span[3] < start_ts:\n",
    "            start_ts = span[3]\n",
    "        if end_ts is None or end_ts < span[4]:\n",
    "            end_ts = span[4]\n",
    "\n",
    "    if norm_topo not in e2e_latency:\n",
    "        e2e_latency[norm_topo] = [(start_ts, end_ts, ret_code)]\n",
    "    else:\n",
    "        e2e_latency[norm_topo].append((start_ts, end_ts, ret_code))  \n",
    "\n",
    "print(f\"Error code set: {error_code_set}\")\n",
    "\n",
    "print(f\"Trace count: {len(e2e_latency)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "e2e_anomalies = []\n",
    "e2e_stat = {}\n",
    "\n",
    "print(len(e2e_latency))\n",
    "for norm_topo in e2e_latency.keys():\n",
    "    durations = []\n",
    "    for entry in e2e_latency[norm_topo]:\n",
    "        duration = (entry[1] - entry[0]).total_seconds()\n",
    "        durations.append(duration)\n",
    "    mean, std = np.mean(durations), np.std(durations)\n",
    "    try:\n",
    "        assert mean > 0\n",
    "    except AssertionError:\n",
    "        print(f\"Anomaly: {norm_topo}, {mean}, {std}\")\n",
    "        print(e2e_latency[norm_topo])\n",
    "        print(durations)\n",
    "        raise AssertionError\n",
    "    e2e_stat[norm_topo] = (mean, std)\n",
    "\n",
    "for norm_topo in e2e_latency.keys():\n",
    "    mean, std = e2e_stat[norm_topo]\n",
    "    for entry in e2e_latency[norm_topo]:\n",
    "        if entry[2] != 200 and entry[2] != 300:\n",
    "            e2e_anomalies.append((entry[0], entry[1]))\n",
    "            continue\n",
    "\n",
    "        duration = (entry[1] - entry[0]).total_seconds()\n",
    "        if duration <= mean - 3 * std or mean + 3 * std <= duration:\n",
    "            e2e_anomalies.append((entry[0], entry[1]))\n",
    "\n",
    "e2e_anomalies = sorted(e2e_anomalies, key=lambda x: x[0])\n",
    "print(len(e2e_anomalies))\n",
    "for anomaly in e2e_anomalies:\n",
    "    print(anomaly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del trace_dict\n",
    "del error_code_set\n",
    "del e2e_latency\n",
    "del e2e_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve Metric Anomaly Symptom from metric files\n",
    "\n",
    "Grountruth anomaly log files are from `../../Datasets/dataset2/data/metric/*.csv`.\n",
    "The following scripts generates an **metric_anomalies** dict.\n",
    "Each of its elements is `{key: metric_name, value: (start_timestamp, end_timestamp)}`.\n",
    "Anomalies are judged by 3-sigma principle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read metric first\n",
    "metric_dir = \"../../Datasets/dataset2/data/metric/\"\n",
    "metric_anomalies = {}\n",
    "metric_interval = {}\n",
    "\n",
    "for metric_file in list_files(metric_dir):\n",
    "    metric_name = \"_\".join(os.path.basename(metric_file).split('_')[:-2])\n",
    "    raw_metric_list = []\n",
    "    print(f\"Processing metric: {metric_name}\")\n",
    "\n",
    "    for row in read_large_csv(metric_file):\n",
    "        ts, val = datetime.fromtimestamp(float(row[0]) / 1000), float(row[1])\n",
    "        raw_metric_list.append((ts, val))\n",
    "\n",
    "    raw_metric_list = sorted(raw_metric_list, key=lambda x: x[0])\n",
    "    raw_metric_val = [val for _, val in raw_metric_list]\n",
    "    mean, std = np.mean(raw_metric_val), np.std(raw_metric_val)\n",
    "    print(f\"Mean: {mean}, Std: {std}, Min: {min(raw_metric_val)}, Max: {max(raw_metric_val)}, Length: {len(raw_metric_val)}\")\n",
    "\n",
    "    last_ts, interval = None, None\n",
    "    for ts, val in raw_metric_list:\n",
    "        if last_ts is not None:\n",
    "            if interval is None or interval == 0:\n",
    "                interval = (ts - last_ts).total_seconds()\n",
    "            else:\n",
    "                try:\n",
    "                    assert interval == (ts - last_ts).total_seconds()\n",
    "                except AssertionError:\n",
    "                    print(f\"Interval mismatch: origin interval {interval}, current interval {(ts - last_ts).total_seconds()}, current {ts} - {ts.timestamp()}, last {last_ts} - {last_ts.timestamp()}\")\n",
    "        last_ts = ts\n",
    "\n",
    "        if val <= mean - 3 * std or mean + 3 * std <= val:\n",
    "            print(f\"Anomaly: {ts} - {val}\")\n",
    "            if metric_name not in metric_anomalies:\n",
    "                metric_anomalies[metric_name] = [ts]\n",
    "            else:\n",
    "                metric_anomalies[metric_name].append(ts)\n",
    "\n",
    "    if metric_name not in metric_anomalies:\n",
    "        print(f\"No anomaly found in {metric_name}\")\n",
    "        print(f\"Mean: {mean}, Std: {std}, Min: {min(raw_metric_val)}, Max: {max(raw_metric_val)}\")\n",
    "    else:\n",
    "        print(f\"Metric: {metric_name}, from total {len(raw_metric_val)} values, {len(metric_anomalies[metric_name])} anomalies found.\")\n",
    "\n",
    "# print(f\"Trace count: {len(trace_dict)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".pyvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
