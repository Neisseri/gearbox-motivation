{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSV Reader\n",
    "\n",
    "`def read_large_csv(file_path)` parse CSV line by line.\n",
    "To use it, run `for row in read_large_csv(xxx):`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def read_large_csv(file_path):\n",
    "    with open(file_path, mode='r', encoding='utf-8') as f:\n",
    "        reader = csv.reader(f)\n",
    "        header = next(reader)\n",
    "        print(f\"CSV Header of {file_path}: {header}\")\n",
    "        for row in reader:\n",
    "            yield row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve Anomaly Groudtruth from log files\n",
    "\n",
    "Grountruth anomaly log files are from [../../Datasets/dataset2/data/run/run_table_2021-07.csv](../../Datasets/dataset2/data/run/run_table_2021-07.csv) and [../../Datasets/dataset2/data/run/run_table_2021-08.csv](../../Datasets/dataset2/data/run/run_table_2021-08.csv).\n",
    "The following scripts generates an **anomalies** list.\n",
    "Each of its elements is `(anomaly_type, start_timestamp, end_timestamp, duration)`.\n",
    "`anomaly_type` contains `Mem`, `Single Trace Wait`, `File`, `CPU`, `Access`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV Header of ../../Datasets/dataset2/data/run/run_table_2021-07.csv: ['datetime', 'service', 'message']\n",
      "The anomaly is too long, ignored: 1631517.6494772434s = 453.1993470770121h = 18.883306128208837d\n",
      "The anomaly is too long, ignored: 1985016.0505759716s = 551.3933473822143h = 22.97472280759226d\n",
      "The anomaly is too long, ignored: 1996721.3545985222s = 554.6448207218117h = 23.11020086340882d\n",
      "CSV Header of ../../Datasets/dataset2/data/run/run_table_2021-08.csv: ['', 'datetime', 'service', 'message']\n",
      "The anomaly is too long, ignored: 2319736.971086979s = 644.3713808574942h = 26.848807535728923d\n",
      "The anomaly is too long, ignored: 2476386.2864255905s = 687.885079562664h = 28.661878315111d\n",
      "The anomaly is too long, ignored: 2654764.702501297s = 737.4346395836935h = 30.72644331598723d\n",
      "The anomaly is too long, ignored: 2684473.170890093s = 745.6869919139148h = 31.07029132974645d\n",
      "The anomaly is too long, ignored: 1443225.1516616344s = 400.89587546156514h = 16.703994810898546d\n",
      "Anomaly count: 34284, 34247 anomalies found.\n"
     ]
    }
   ],
   "source": [
    "# Retrieve Anomaly Groudtruth from the log files\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "injection_log_1 = \"../../Datasets/dataset2/data/run/run_table_2021-07.csv\"\n",
    "injection_log_2 = \"../../Datasets/dataset2/data/run/run_table_2021-08.csv\"\n",
    "\n",
    "# anomaly_types: Mem, Single Trace Wait, File, CPU, Access\n",
    "# anomalies = [(service_name, anomaly_type, start_timestamp, end_timestamp, duration), ...]\n",
    "anomalies = []\n",
    "anomaly_raw_cnt = 0\n",
    "\n",
    "for row in read_large_csv(injection_log_1):\n",
    "    log_entry = row[-1].split(' | ')\n",
    "    service_name = row[1]\n",
    "\n",
    "    # These logs are mostly '(Background on this error at: http://sqlalche.me/e/14/e3q8)\\n'\n",
    "    if len(log_entry) < 2:\n",
    "        continue\n",
    "\n",
    "    log_type = log_entry[1]\n",
    "    if log_type == \"INFO\" or log_type == \"ERROR\":\n",
    "        continue\n",
    "    if log_type != \"WARNING\":\n",
    "        raise Exception(f\"Unknown log type: {log_type}\")\n",
    "\n",
    "    anomaly_raw_cnt += 1\n",
    "    if \"[memory_anomalies]\" in log_entry[-1]:\n",
    "        anomaly_info = log_entry[-1]\n",
    "        anomaly_type = \"Mem\"\n",
    "\n",
    "        # 提取开始时间\n",
    "        start_indicator = \"start at \"\n",
    "        start_idx = anomaly_info.index(start_indicator) + len(start_indicator)\n",
    "        end_idx = anomaly_info.index(\" and lasts\", start_idx)\n",
    "        timestamp_str = anomaly_info[start_idx:end_idx]\n",
    "        start_ts = datetime.strptime(timestamp_str, \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "\n",
    "        lasts_indicator = \"lasts \"\n",
    "        lasts_idx = anomaly_info.index(lasts_indicator) + len(lasts_indicator)\n",
    "        lasts_end_idx = anomaly_info.index(\" seconds\", lasts_idx)\n",
    "        lasts_str = anomaly_info[lasts_idx:lasts_end_idx]\n",
    "        duration = float(lasts_str)\n",
    "\n",
    "        end_ts = start_ts + timedelta(seconds=duration)\n",
    "\n",
    "        # print(log_entry)\n",
    "        # print(f\"Anomaly: {start_ts} - {end_ts} ({duration}s)\")\n",
    "        anomalies.append((service_name, anomaly_type, start_ts, end_ts, duration))\n",
    "    elif \"[normal memory freed label]\" in log_entry[-1]:\n",
    "        continue\n",
    "    elif \"simulate the login failure of the QR code expired\" in log_entry[-1]:\n",
    "        anomaly_info = log_entry[-1]\n",
    "        anomaly_type = \"Single Trace Wait\"\n",
    "\n",
    "        # 提取开始时间\n",
    "        timestamp_sec_str = log_entry[0].split(',')[0]\n",
    "        start_ts = datetime.strptime(timestamp_sec_str, \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "        lasts_indicator = \"wait for \"\n",
    "        lasts_idx = anomaly_info.index(lasts_indicator) + len(lasts_indicator)\n",
    "        lasts_end_idx = anomaly_info.index(\" seconds\", lasts_idx)\n",
    "        lasts_str = anomaly_info[lasts_idx:lasts_end_idx]\n",
    "        duration = float(lasts_str)\n",
    "\n",
    "        end_ts = start_ts + timedelta(seconds=duration)\n",
    "\n",
    "        # print(log_entry)\n",
    "        # print(f\"Anomaly: {start_ts} - {end_ts} ({duration}s)\")\n",
    "        anomalies.append((service_name, anomaly_type, start_ts, end_ts, duration))\n",
    "    elif \"trigger the file moving program\" in log_entry[-1]:\n",
    "        anomaly_info = log_entry[-1]\n",
    "        anomaly_type = \"File\"\n",
    "\n",
    "        # 提取开始时间\n",
    "        start_indicator = \"start with \"\n",
    "        start_idx = anomaly_info.index(start_indicator) + len(start_indicator)\n",
    "        end_idx = anomaly_info.index(\", last for\", start_idx)\n",
    "        timestamp_str = anomaly_info[start_idx:end_idx]\n",
    "        start_ts = datetime.strptime(timestamp_str, \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "\n",
    "        lasts_indicator = \"last for \"\n",
    "        lasts_idx = anomaly_info.index(lasts_indicator) + len(lasts_indicator)\n",
    "        lasts_end_idx = anomaly_info.index(\" seconds\", lasts_idx)\n",
    "        lasts_str = anomaly_info[lasts_idx:lasts_end_idx]\n",
    "        duration = float(lasts_str)\n",
    "\n",
    "        end_ts = start_ts + timedelta(seconds=duration)\n",
    "\n",
    "        # print(log_entry)\n",
    "        # print(f\"Anomaly: {start_ts} - {end_ts} ({duration}s)\")\n",
    "        anomalies.append((service_name, anomaly_type, start_ts, end_ts, duration))\n",
    "    elif \"[cpu_anomalies]\" in log_entry[-1]:\n",
    "        anomaly_info = log_entry[-1]\n",
    "        anomaly_type = \"CPU\"\n",
    "\n",
    "        # 提取开始时间\n",
    "        start_indicator = \"start at \"\n",
    "        start_idx = anomaly_info.index(start_indicator) + len(start_indicator)\n",
    "        end_idx = anomaly_info.index(\" and lasts\", start_idx)\n",
    "        timestamp_str = anomaly_info[start_idx:end_idx]\n",
    "        start_ts = datetime.strptime(timestamp_str, \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "\n",
    "        lasts_indicator = \"lasts \"\n",
    "        lasts_idx = anomaly_info.index(lasts_indicator) + len(lasts_indicator)\n",
    "        lasts_end_idx = anomaly_info.index(\" seconds\", lasts_idx)\n",
    "        lasts_str = anomaly_info[lasts_idx:lasts_end_idx]\n",
    "        duration = float(lasts_str)\n",
    "\n",
    "        end_ts = start_ts + timedelta(seconds=duration)\n",
    "\n",
    "        # print(log_entry)\n",
    "        # print(f\"Anomaly: {start_ts} - {end_ts} ({duration}s)\")\n",
    "        if duration > 24 * 60 * 60 * 14:\n",
    "            print(f\"The anomaly is too long, ignored: {duration}s = {duration / 60 / 60}h = {duration / 60 / 60 / 24}d\")\n",
    "            continue\n",
    "        anomalies.append((service_name, anomaly_type, start_ts, end_ts, duration))\n",
    "    elif \"trigger an access permission denied exception, will lasts an hour\" in log_entry[-1]:\n",
    "        anomaly_type = \"Access\"\n",
    "\n",
    "        # 提取开始时间\n",
    "        timestamp_sec_str = log_entry[0].split(',')[0]\n",
    "        start_ts = datetime.strptime(timestamp_sec_str, \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "        duration = 60 * 60  # 1 Hour\n",
    "\n",
    "        end_ts = start_ts + timedelta(seconds=duration)        \n",
    "        # print(log_entry)\n",
    "        # print(f\"Anomaly: {start_ts} - {end_ts} ({duration}s)\")\n",
    "        anomalies.append((service_name, anomaly_type, start_ts, end_ts, duration))\n",
    "    else:\n",
    "        print(log_entry)\n",
    "\n",
    "for row in read_large_csv(injection_log_2):\n",
    "    log_entry = row[-1].split(' | ')\n",
    "\n",
    "    # These logs are mostly '(Background on this error at: http://sqlalche.me/e/14/e3q8)\\n'\n",
    "    if len(log_entry) < 2:\n",
    "        continue\n",
    "\n",
    "    log_type = log_entry[1]\n",
    "    if log_type == \"INFO\" or log_type == \"ERROR\":\n",
    "        continue\n",
    "    if log_type != \"WARNING\":\n",
    "        raise Exception(f\"Unknown log type: {log_type}\")\n",
    "\n",
    "    anomaly_raw_cnt += 1    \n",
    "    if \"[memory_anomalies]\" in log_entry[-1]:\n",
    "        anomaly_info = log_entry[-1]\n",
    "        anomaly_type = \"Mem\"\n",
    "\n",
    "        # 提取开始时间\n",
    "        start_indicator = \"start at \"\n",
    "        start_idx = anomaly_info.index(start_indicator) + len(start_indicator)\n",
    "        end_idx = anomaly_info.index(\" and lasts\", start_idx)\n",
    "        timestamp_str = anomaly_info[start_idx:end_idx]\n",
    "        start_ts = datetime.strptime(timestamp_str, \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "\n",
    "        lasts_indicator = \"lasts \"\n",
    "        lasts_idx = anomaly_info.index(lasts_indicator) + len(lasts_indicator)\n",
    "        lasts_end_idx = anomaly_info.index(\" seconds\", lasts_idx)\n",
    "        lasts_str = anomaly_info[lasts_idx:lasts_end_idx]\n",
    "        duration = float(lasts_str)\n",
    "\n",
    "        end_ts = start_ts + timedelta(seconds=duration)\n",
    "\n",
    "        # print(log_entry)\n",
    "        # print(f\"Anomaly: {start_ts} - {end_ts} ({duration}s)\")\n",
    "        anomalies.append((service_name, anomaly_type, start_ts, end_ts, duration))\n",
    "    elif \"[normal memory freed label]\" in log_entry[-1]:\n",
    "        continue\n",
    "    elif \"simulate the login failure of the QR code expired\" in log_entry[-1]:\n",
    "        anomaly_info = log_entry[-1]\n",
    "        anomaly_type = \"Single Trace Wait\"\n",
    "\n",
    "        # 提取开始时间\n",
    "        timestamp_sec_str = log_entry[0].split(',')[0]\n",
    "        start_ts = datetime.strptime(timestamp_sec_str, \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "        lasts_indicator = \"wait for \"\n",
    "        lasts_idx = anomaly_info.index(lasts_indicator) + len(lasts_indicator)\n",
    "        lasts_end_idx = anomaly_info.index(\" seconds\", lasts_idx)\n",
    "        lasts_str = anomaly_info[lasts_idx:lasts_end_idx]\n",
    "        duration = float(lasts_str)\n",
    "\n",
    "        end_ts = start_ts + timedelta(seconds=duration)\n",
    "\n",
    "        # print(log_entry)\n",
    "        # print(f\"Anomaly: {start_ts} - {end_ts} ({duration}s)\")\n",
    "        anomalies.append((service_name, anomaly_type, start_ts, end_ts, duration))\n",
    "    elif \"trigger the file moving program\" in log_entry[-1]:\n",
    "        anomaly_info = log_entry[-1]\n",
    "        anomaly_type = \"File\"\n",
    "\n",
    "        # 提取开始时间\n",
    "        start_indicator = \"start with \"\n",
    "        start_idx = anomaly_info.index(start_indicator) + len(start_indicator)\n",
    "        end_idx = anomaly_info.index(\", last for\", start_idx)\n",
    "        timestamp_str = anomaly_info[start_idx:end_idx]\n",
    "        start_ts = datetime.strptime(timestamp_str, \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "\n",
    "        lasts_indicator = \"last for \"\n",
    "        lasts_idx = anomaly_info.index(lasts_indicator) + len(lasts_indicator)\n",
    "        lasts_end_idx = anomaly_info.index(\" seconds\", lasts_idx)\n",
    "        lasts_str = anomaly_info[lasts_idx:lasts_end_idx]\n",
    "        duration = float(lasts_str)\n",
    "\n",
    "        end_ts = start_ts + timedelta(seconds=duration)\n",
    "\n",
    "        # print(log_entry)\n",
    "        # print(f\"Anomaly: {start_ts} - {end_ts} ({duration}s)\")\n",
    "        anomalies.append((service_name, anomaly_type, start_ts, end_ts, duration))\n",
    "    elif \"[cpu_anomalies]\" in log_entry[-1]:\n",
    "        anomaly_info = log_entry[-1]\n",
    "        anomaly_type = \"CPU\"\n",
    "\n",
    "        # 提取开始时间\n",
    "        start_indicator = \"start at \"\n",
    "        start_idx = anomaly_info.index(start_indicator) + len(start_indicator)\n",
    "        end_idx = anomaly_info.index(\" and lasts\", start_idx)\n",
    "        timestamp_str = anomaly_info[start_idx:end_idx]\n",
    "        start_ts = datetime.strptime(timestamp_str, \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "\n",
    "        lasts_indicator = \"lasts \"\n",
    "        lasts_idx = anomaly_info.index(lasts_indicator) + len(lasts_indicator)\n",
    "        lasts_end_idx = anomaly_info.index(\" seconds\", lasts_idx)\n",
    "        lasts_str = anomaly_info[lasts_idx:lasts_end_idx]\n",
    "        duration = float(lasts_str)\n",
    "\n",
    "        end_ts = start_ts + timedelta(seconds=duration)\n",
    "\n",
    "        # print(log_entry)\n",
    "        # print(f\"Anomaly: {start_ts} - {end_ts} ({duration}s)\")\n",
    "        if duration > 24 * 60 * 60 * 14:\n",
    "            print(f\"The anomaly is too long, ignored: {duration}s = {duration / 60 / 60}h = {duration / 60 / 60 / 24}d\")\n",
    "            continue\n",
    "        anomalies.append((service_name, anomaly_type, start_ts, end_ts, duration))\n",
    "    elif \"trigger an access permission denied exception, will lasts an hour\" in log_entry[-1]:\n",
    "        anomaly_type = \"Access\"\n",
    "\n",
    "        # 提取开始时间\n",
    "        timestamp_sec_str = log_entry[0].split(',')[0]\n",
    "        start_ts = datetime.strptime(timestamp_sec_str, \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "        duration = 60 * 60  # 1 Hour\n",
    "\n",
    "        end_ts = start_ts + timedelta(seconds=duration)        \n",
    "        # print(log_entry)\n",
    "        # print(f\"Anomaly: {start_ts} - {end_ts} ({duration}s)\")\n",
    "        anomalies.append((service_name, anomaly_type, start_ts, end_ts, duration))\n",
    "    else:\n",
    "        print(log_entry)\n",
    "\n",
    "print(f\"Anomaly count: {anomaly_raw_cnt}, {len(anomalies)} anomalies found.\")\n",
    "# for anomaly in anomalies:\n",
    "#     print(anomaly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "363"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import gc\n",
    "\n",
    "# anomalies: [service_name, anomaly_type, start_ts, end_ts, duration]\n",
    "data = {\n",
    "    'anomaly_raw_cnt': anomaly_raw_cnt,\n",
    "    'anomalies': anomalies\n",
    "}\n",
    "\n",
    "with open('anomalies.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=4, default=str)\n",
    "\n",
    "del anomaly_raw_cnt\n",
    "del anomalies\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve End-to-end Anomaly Symptom from trace files\n",
    "\n",
    "Grountruth anomaly log files are from `../../Datasets/dataset2/data/trace/*.csv`.\n",
    "The following scripts generates an **e2e_anomalies** list.\n",
    "Each of its elements is `(start_timestamp, end_timestamp)`.\n",
    "Anomalies are judged by 3-sigma principle ($\\mu - 3 * \\sigma \\leq value \\leq \\mu + 3 * \\sigma$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../../Datasets/dataset2/data/trace/trace_table_dbservice2_2021-07.csv\n",
      "CSV Header of ../../Datasets/dataset2/data/trace/trace_table_dbservice2_2021-07.csv: ['timestamp', 'host_ip', 'service_name', 'trace_id', 'span_id', 'parent_id', 'start_time', 'end_time', 'url', 'status_code', 'message']\n",
      "Processed 1000000 rows\n",
      "Processing ../../Datasets/dataset2/data/trace/trace_table_mobservice1_2021-07.csv\n",
      "CSV Header of ../../Datasets/dataset2/data/trace/trace_table_mobservice1_2021-07.csv: ['timestamp', 'host_ip', 'service_name', 'trace_id', 'span_id', 'parent_id', 'start_time', 'end_time', 'url', 'status_code', 'message']\n",
      "Processed 1000000 rows\n",
      "Processing ../../Datasets/dataset2/data/trace/trace_table_logservice1_2021-07.csv\n",
      "CSV Header of ../../Datasets/dataset2/data/trace/trace_table_logservice1_2021-07.csv: ['timestamp', 'host_ip', 'service_name', 'trace_id', 'span_id', 'parent_id', 'start_time', 'end_time', 'url', 'status_code', 'message']\n",
      "Processed 1000000 rows\n",
      "Processed 2000000 rows\n",
      "Processing ../../Datasets/dataset2/data/trace/trace_table_webservice1_2021-07.csv\n",
      "CSV Header of ../../Datasets/dataset2/data/trace/trace_table_webservice1_2021-07.csv: ['timestamp', 'host_ip', 'service_name', 'trace_id', 'span_id', 'parent_id', 'start_time', 'end_time', 'url', 'status_code', 'message']\n",
      "Processed 1000000 rows\n",
      "Processing ../../Datasets/dataset2/data/trace/trace_table_mobservice2_2021-07.csv\n",
      "CSV Header of ../../Datasets/dataset2/data/trace/trace_table_mobservice2_2021-07.csv: ['timestamp', 'host_ip', 'service_name', 'trace_id', 'span_id', 'parent_id', 'start_time', 'end_time', 'url', 'status_code', 'message']\n",
      "Processed 1000000 rows\n",
      "Processing ../../Datasets/dataset2/data/trace/trace_table_dbservice1_2021-07.csv\n",
      "CSV Header of ../../Datasets/dataset2/data/trace/trace_table_dbservice1_2021-07.csv: ['timestamp', 'host_ip', 'service_name', 'trace_id', 'span_id', 'parent_id', 'start_time', 'end_time', 'url', 'status_code', 'message']\n",
      "Processed 1000000 rows\n",
      "Processing ../../Datasets/dataset2/data/trace/trace_table_redisservice1_2021-07.csv\n",
      "CSV Header of ../../Datasets/dataset2/data/trace/trace_table_redisservice1_2021-07.csv: ['timestamp', 'host_ip', 'service_name', 'trace_id', 'span_id', 'parent_id', 'start_time', 'end_time', 'url', 'status_code', 'message']\n",
      "Processed 1000000 rows\n",
      "Processed 2000000 rows\n",
      "Processed 3000000 rows\n",
      "Processed 4000000 rows\n",
      "Processed 5000000 rows\n",
      "Processed 6000000 rows\n",
      "Processed 7000000 rows\n",
      "Processing ../../Datasets/dataset2/data/trace/trace_table_logservice2_2021-07.csv\n",
      "CSV Header of ../../Datasets/dataset2/data/trace/trace_table_logservice2_2021-07.csv: ['timestamp', 'host_ip', 'service_name', 'trace_id', 'span_id', 'parent_id', 'start_time', 'end_time', 'url', 'status_code', 'message']\n",
      "Processed 1000000 rows\n",
      "Processed 2000000 rows\n",
      "Processing ../../Datasets/dataset2/data/trace/trace_table_webservice2_2021-07.csv\n",
      "CSV Header of ../../Datasets/dataset2/data/trace/trace_table_webservice2_2021-07.csv: ['timestamp', 'host_ip', 'service_name', 'trace_id', 'span_id', 'parent_id', 'start_time', 'end_time', 'url', 'status_code', 'message']\n",
      "Processed 1000000 rows\n",
      "Processing ../../Datasets/dataset2/data/trace/trace_table_redisservice2_2021-07.csv\n",
      "CSV Header of ../../Datasets/dataset2/data/trace/trace_table_redisservice2_2021-07.csv: ['timestamp', 'host_ip', 'service_name', 'trace_id', 'span_id', 'parent_id', 'start_time', 'end_time', 'url', 'status_code', 'message']\n",
      "Processed 1000000 rows\n",
      "Processed 2000000 rows\n",
      "Processed 3000000 rows\n",
      "Processed 4000000 rows\n",
      "Processed 5000000 rows\n",
      "Processed 6000000 rows\n",
      "Processed 7000000 rows\n",
      "Trace count: 3084066\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "def list_files(root_dir):\n",
    "    for root, dirs, files in os.walk(root_dir):\n",
    "        for file in files:\n",
    "            yield os.path.join(root, file)\n",
    "\n",
    "# Generate trace first\n",
    "trace_dir = \"../../Datasets/dataset2/data/trace/\"\n",
    "trace_dict = {}\n",
    "for trace_file in list_files(trace_dir):\n",
    "    print(f\"Processing {trace_file}\")\n",
    "    row_count = 0\n",
    "    for row in read_large_csv(trace_file):\n",
    "        row_count += 1\n",
    "        if row_count % 1000000 == 0:\n",
    "            print(f\"Processed {row_count} rows\")\n",
    "\n",
    "        service_name = row[2]\n",
    "        trace_id = row[3]\n",
    "        span_id = row[4]\n",
    "        parent_span_id = row[5]\n",
    "        try:\n",
    "            start_ts = datetime.strptime(row[6], \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "        except ValueError:\n",
    "            start_ts = datetime.strptime(row[6], \"%Y-%m-%d %H:%M:%S\")\n",
    "        try:\n",
    "            end_ts = datetime.strptime(row[7], \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "        except ValueError:\n",
    "            end_ts = datetime.strptime(row[7], \"%Y-%m-%d %H:%M:%S\")\n",
    "        status_code = int(row[9])\n",
    "\n",
    "        if trace_id not in trace_dict:\n",
    "            trace_dict[trace_id] = [(service_name, span_id, parent_span_id, start_ts, end_ts, status_code)]\n",
    "        else:\n",
    "            trace_dict[trace_id].append((service_name, span_id, parent_span_id, start_ts, end_ts, status_code))            \n",
    "\n",
    "print(f\"Trace count: {len(trace_dict)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import gc\n",
    "\n",
    "# trace_dict[trace_id] = [(service_name, span_id, parent_span_id, start_ts, end_ts, status_code), (), ...]\n",
    "data = {\n",
    "    'trace_count': len(trace_dict),\n",
    "    'trace_dict': trace_dict\n",
    "}\n",
    "\n",
    "with open('trace_dict.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=4, default=str)\n",
    "\n",
    "del trace_dict\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "# import pickle\n",
    "# import gzip\n",
    "# import gc\n",
    "\n",
    "# # trace_dict[trace_id] = [(service_name, span_id, parent_span_id, start_ts, end_ts, status_code), (), ...]\n",
    "# with gzip.open('trace_dict.pkl.gz', 'wb') as f:\n",
    "#     pickle.dump(trace_dict, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# # # 从文件中读取数据\n",
    "# # with gzip.open('trace_dict.pkl.gz', 'rb') as f:\n",
    "# #     loaded_trace_dict = pickle.load(f)\n",
    "\n",
    "# # # 打印读取的数据\n",
    "# # print(\"Loaded trace_dict:\", loaded_trace_dict)\n",
    "\n",
    "# del trace_dict\n",
    "\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error code set: {200, 400, 300, 500}\n",
      "Trace count: 364\n"
     ]
    }
   ],
   "source": [
    "def build_topology(span_list):\n",
    "    def add_span_to_topology(topology, span):\n",
    "        service, span_id, parent_id, _, _, _ = span\n",
    "        if parent_id not in topology:\n",
    "            topology[parent_id] = {'children': [], 'service': \"Start\"}\n",
    "        if span_id not in topology:\n",
    "            topology[span_id] = {'service': service, 'children': []}\n",
    "        topology[parent_id]['children'].append(span_id)\n",
    "        topology[span_id]['service'] = service\n",
    "\n",
    "    topology = {}\n",
    "    for span in span_list:\n",
    "        add_span_to_topology(topology, span)\n",
    "    return topology\n",
    "\n",
    "def normalize_topology(topology, root_id='0'):\n",
    "    \"\"\"\n",
    "    Standardize the topology for comparison by normalizing the ordering.\n",
    "    \"\"\"\n",
    "    def recursive_normalize(node):\n",
    "        if 'service' in node:\n",
    "            node_str = node['service']\n",
    "        else:\n",
    "            node_str = \"\"\n",
    "        children_strs = []\n",
    "        for child_id in node['children']:\n",
    "            if child_id in topology:\n",
    "                child_str = recursive_normalize(topology[child_id])\n",
    "                children_strs.append(child_str)\n",
    "        children_strs.sort()\n",
    "        complete_str = node_str + \"(\" + \",\".join(children_strs) + \")\"\n",
    "        return complete_str\n",
    "\n",
    "    root_topology = topology.get(root_id, {'service': '', 'children': []})\n",
    "    return recursive_normalize(root_topology)\n",
    "\n",
    "e2e_latency = dict()\n",
    "error_code_set = set()\n",
    "# trace_dict[trace_id] = [(service_name, span_id, parent_span_id, start_ts, end_ts, status_code)]\n",
    "for trace_id, spans in trace_dict.items():\n",
    "    topo = build_topology(spans)\n",
    "    norm_topo = normalize_topology(topo)\n",
    "    start_ts, end_ts, ret_code = None, None, None\n",
    "    for span in spans:\n",
    "        # print(span)\n",
    "        error_code_set.add(span[-1])\n",
    "\n",
    "        if ret_code is None or ret_code < span[-1]: # status code\n",
    "            ret_code = span[-1]\n",
    "        if start_ts is None or span[3] < start_ts: # start_ts\n",
    "            start_ts = span[3]\n",
    "        if end_ts is None or end_ts < span[4]: # end_ts\n",
    "            end_ts = span[4]\n",
    "\n",
    "    if norm_topo not in e2e_latency:\n",
    "        e2e_latency[norm_topo] = [(trace_id, start_ts, end_ts, ret_code)]\n",
    "    else:\n",
    "        e2e_latency[norm_topo].append((trace_id, start_ts, end_ts, ret_code))  \n",
    "\n",
    "print(f\"Error code set: {error_code_set}\")\n",
    "\n",
    "print(f\"Trace count: {len(e2e_latency)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
